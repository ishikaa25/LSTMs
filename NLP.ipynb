{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('myenv': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "interpreter": {
      "hash": "03dd13d48678367c8c9c8d2bc4e4058efaffff37d2a70d9886c86b6aa2328a71"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# tv = TfidfVectorizer(use_idf=True) \n",
        " \n",
        "# just send in all your docs here \n",
        "# tv_vectors=tv.fit_transform(x_train)\n",
        "# from sklearn.feature_selection import SelectKBest,f_classif\n",
        "# fvalue_Best = SelectKBest(f_classif, k=200)\n",
        "# X = fvalue_Best.fit_transform(tv_vectors,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Glr5_8BtE8Q_"
      },
      "source": [
        "# from nltk.stem import WordNetLemmatizer\n",
        "# nltk.download('wordnet')\n",
        "# import torchtext\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from pytictoc import TicToc"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZctBATa6iGG",
        "outputId": "455835ed-6002-4352-d00b-f1619f7e9386"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words()\n",
        "t = TicToc()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /home/ishikaa/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/ishikaa/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK9sPPWF64KR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "03ab122f-53b9-4b8f-b193-8a088ca3c115"
      },
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkDrpkhtXoKu"
      },
      "source": [
        "X,y = df['review'].values,df['sentiment'].values\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PdrmHtEP2qt"
      },
      "source": [
        "def preprocess(data):\n",
        "  '''\n",
        "  Input Pandas series\n",
        "  '''\n",
        "\n",
        "  #To LOWERCASE\n",
        "  text = [ex.lower() for ex in data]\n",
        "\n",
        "  #Remove HTML stuff\n",
        "  text= [re.sub(\"(<.*?>)\",\"\",ex) for ex in text]  \n",
        "  #remove non-ascii and digits\n",
        "  text= [re.sub(\"(\\\\W|\\\\d)\",\" \",ex) for ex in text]  \n",
        "    \n",
        "  #Remove whitespace\n",
        "  text= [ex.strip() for ex in text]\n",
        "  #Remove multiple spaces\n",
        "  text = [re.sub(' +', ' ',ex) for ex in text]\n",
        "\n",
        "  #Remove stop words\n",
        "  text = list(map(lambda words: ' '.join(word for word in words.split() if word not in stop),text))\n",
        "\n",
        "  return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpqEbKJ3XstB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9413eb-596d-4204-9a2e-846f050994f2"
      },
      "source": [
        "t.tic()\n",
        "x_train = preprocess(X_train)\n",
        "x_test = preprocess(X_test)\n",
        "t.toc()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time is 479.935432 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x_train_pre = x_train\n",
        "# x_test_pre = x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = np.asarray([0 if y=='positive' else 1 for y in y_train])\n",
        "y_test = np.asarray([0 if y=='positive' else 1 for y in y_test])"
      ]
    },
    {
      "source": [
        "## Some random shiz"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = x_train_pre\n",
        "X_test = x_test_pre"
      ]
    },
    {
      "source": [
        "from collections import Counter"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4TaGhe8V8LlM"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUeQJVOF8PRn"
      },
      "source": [
        "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
        "for i, sentence in enumerate(X_train):\n",
        "    # The sentences will be stored as a list of words/tokens\n",
        "    X_train[i] = []\n",
        "    # print(type(sentence))\n",
        "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
        "        words.update([word])\n",
        "        X_train[i].append(word)\n",
        "    if i%5000 == 0:\n",
        "      print(str(i) + \"done\")\n",
        "\n",
        "print(\"100% done\")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0done\n",
            "5000done\n",
            "10000done\n",
            "15000done\n",
            "20000done\n",
            "25000done\n",
            "30000done\n",
            "35000done\n",
            "100% done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehxu_3bD8fBO"
      },
      "source": [
        "words = {k:v for k,v in words.items() if v>100}\n",
        "words = sorted(words,key=words.get,reverse=True)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T28piOG--5LL"
      },
      "source": [
        "words = ['_PAD','_UNK'] + words"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EFoEqcF_QmO"
      },
      "source": [
        "word2idx = {o:i for i,o in enumerate(words)}\n",
        "idx2word = {i:o for i,o in enumerate(words)}"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7amNR9B_VGO"
      },
      "source": [
        "for idx, sentence in enumerate(X_train):\n",
        "  X_train[idx] = [word2idx[word] if word in word2idx else 0 for word in sentence]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwZRQHh5DO8i"
      },
      "source": [
        "def alter_length(sentences,length):\n",
        "  new = np.zeros((len(sentences),length))\n",
        "\n",
        "  for idx,sentence in enumerate(sentences):\n",
        "    # print(idx)\n",
        "    if len(sentence)!=0:\n",
        "      new[idx,-len(sentence):] = np.array(sentence)[:length]\n",
        "    \n",
        "  return new"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3dyk5EEDwgU"
      },
      "source": [
        "X_train = alter_length(X_train,200) "
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF9JdwlhuKtA",
        "outputId": "9f8ffd00-479f-41f7-c79b-18649e715a31"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(37500, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, sentence in enumerate(X_test):\n",
        "    X_test[idx] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
        "\n",
        "X_test = alter_length(X_test,200) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5393"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "len(word2idx)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(word2idx) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lstm import MyLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "lstm = MyLSTM(vocab=vocab_size,embed=embedding_dim,hidden=hidden_dim,layers=n_layers,out=output_size,                              device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}